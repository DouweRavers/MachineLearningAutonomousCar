import numpy as np

class evaluateNN():
    def __init__(self, X, Y, costFunction):
        np.random.shuffle(X)
        self.X_train, self.X_test, self.X_val = np.split(X, [int 0.6*len(X), 0.8*len(X)])
        np.random.shuffle(Y)
        self.Y_train, self.Y_test, self.Y_val = np.split(Y, [int 0.6*len(Y), 0.8*len(Y)])

        self.costFunction = costFuntion

    def evaluatePol():
        pol_vec = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

        lambda_ = 100

        pol_ideal = 1
        for i in range(len(pol_vec)):
            X_poly = polyFeatures(X, p)
            X_poly, mu, sigma = featureNormalize(X_poly)
            X_poly = np.concatenate([np.ones((self.Y_train.size, 1)), X_poly], axis=1)

            theta_t = trainLinearReg(self.costFunction, X_poly, self.Y_train, lambda_=lambda_, maxiter=55)
            error_val[i], _ = linearRegCostFunction(self.X_val, self.y_val, theta_t, lambda_ = 0)
            
            if (error_val[i] >= error):
                pol_ideal = pol_vec[i]
    
        return pol_ideal
            

    #calculate ideal regularisation term (trained costfunction)
    def evaluateLambda():
        # Selected values of lambda (you should not change this)
        lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]

        # You need to return these variables correctly.
        error_train = np.zeros(len(lambda_vec))
        error_val = np.zeros(len(lambda_vec))

        error_sum = 0
        lambda_ideal = 1
        for i in range(len(lambda_vec)):
            lambda_try = lambda_vec[i]
            theta_t = trainLinearReg(self.costFunction, self.X_train, self.Y_train, lambda_ = lambda_try)
            error_train[i], _ = linearRegCostFunction(self.X_train, self.Y_train, theta_t, lambda_ = 0)
            error_val[i], _ = linearRegCostFunction(self.X_val, self.y_val, theta_t, lambda_ = 0)
            #choose lambda with min(error_train && error_val)
            if (error_train[i] + error_val[i] >= error_sum):
                lambda_ideal = lambda_try

        return lambda_ideal 

    #calculate test-error
    def testError(lambda_ideal):
        theta_t = trainLinearReg(self.costFunction, self.X_train, self.Y_train, lambda_ = lambda_ideal)
        error_test = linearRegCostFunction(self.X_test, self.Y_test, theta_t, lambda_ = 0)

        return error_test




    def featureNormalize(X):
        mu = np.mean(X, axis=0)
        X_norm = X - mu

        sigma = np.std(X_norm, axis=0, ddof=1)
        X_norm /= sigma
        return X_norm

    def polyfeatures(X, p):
        X_poly = np.zeros((X.shape[0], p))

        for i in range(p):
            X_poly[:, i] = X[:, 0] ** (i + 1)

        return X_poly

    def trainLinearReg(linearRegCostFunction, X, y, lambda_=0.0, maxiter=200):
        # Initialize Theta
        initial_theta = np.zeros(X.shape[1])

        # Create "short hand" for the cost function to be minimized
        costFunction = lambda t: linearRegCostFunction(X, y, t, lambda_)

        # Now, costFunction is a function that takes in only one argument
        options = {'maxiter': maxiter}

        # Minimize using scipy
        res = optimize.minimize(costFunction, initial_theta, jac=True, method='TNC', options=options)
        return res.x
}

    